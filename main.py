import nest_asyncio
nest_asyncio.apply()

import csv
import os
import asyncio
from dotenv import load_dotenv
from playwright.async_api import async_playwright, Browser
from tqdm.asyncio import tqdm_asyncio

# 1. åŒ¯å…¥é‡æ§‹å¾Œçš„ Agent å’Œæ–°çš„è³‡æ–™çµæ§‹
from crawler.web_crawler import WebCrawlerAgent
from analyzer.content_analysis import ContentAnalysisAgent
from reporter.report_generation import ReportGenerationAgent
from common.data_structures import AnalysisResult

# --- Configuration ---
CONCURRENT_TASKS = 10 # ç¨å¾®å¢åŠ ä¸¦è¡Œä»»å‹™æ•¸


def load_websites(path: str):
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        # å¢åŠ ä¸€å€‹ç¯©é¸å™¨ï¼Œå¿½ç•¥è¢«è¨»è§£æ‰çš„ URL
        return [(row["URL"], row.get("name", "")) for row in reader if row["URL"] and not row["URL"].startswith('#')]

async def process_website(
    url: str, 
    name: str, 
    crawler: WebCrawlerAgent, 
    analyzer: ContentAnalysisAgent, 
    semaphore: asyncio.Semaphore, 
    browser: Browser
) -> AnalysisResult:
    """è™•ç†å–®ä¸€ç¶²ç«™çš„ Worker Task"""
    async with semaphore:
        try:
            # æµç¨‹ä¸è®Šï¼šçˆ¬å– -> åˆ†æ
            crawl_result = await crawler.crawl(browser, url)
            analysis = await analyzer.analyze(crawl_result)
            return analysis
        except Exception as e:
            print(f"è™•ç† {url} æ™‚ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤: {e}")
            # 2. æ›´æ–°éŒ¯èª¤å›å‚³ï¼Œä»¥åŒ¹é…æ–°çš„ AnalysisResult çµæ§‹
            return AnalysisResult(
                url=url,
                status="ğŸ”¥ éŒ¯èª¤",
                last_updated="N/A",
                score=100,
                notes=f"ä¸»æµç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}",
                broken_links_summary=""
            )

async def main():
    load_dotenv()
    websites = load_websites("config/websites.csv")

    # åˆå§‹åŒ– Agents
    crawler = WebCrawlerAgent()
    analyzer = ContentAnalysisAgent()
    reporter = ReportGenerationAgent()
    
    semaphore = asyncio.Semaphore(CONCURRENT_TASKS)

    async with async_playwright() as p:
        # å•Ÿå‹•å…±äº«çš„ç€è¦½å™¨å¯¦ä¾‹
        browser = await p.chromium.launch(headless=True)
        
        tasks = [
            process_website(url, name, crawler, analyzer, semaphore, browser)
            for url, name in websites
        ]

        print(f"é–‹å§‹åˆ†æ {len(tasks)} å€‹ç¶²ç«™ (ä¸¦è¡Œæ•¸é‡: {CONCURRENT_TASKS})...")
        
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢ä¸¦åŸ·è¡Œæ‰€æœ‰ä»»å‹™
        results = await tqdm_asyncio.gather(*tasks)

        await browser.close()
    
    # 3. åœ¨æ‰€æœ‰æµç¨‹çµæŸå¾Œï¼Œå„ªé›…åœ°é—œé–‰ crawler agent å…§éƒ¨çš„ httpx å®¢æˆ¶ç«¯
    await crawler.http_client.aclose()

    valid_results = [res for res in results if res is not None]

    print(f"\nåˆ†æå®Œæˆï¼Œå…±å–å¾— {len(valid_results)} ç­†çµæœã€‚æ­£åœ¨ç”¢ç”Ÿå ±å‘Š...")
    output_path = reporter.generate(valid_results)
    print(f"å ±å‘Šå·²å„²å­˜è‡³: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())
